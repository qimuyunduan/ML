{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GNB', '\\t--> ', 1.0)\n('RF', '\\t--> ', 0.97666963755199043)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n('AB', '\\t--> ', 0.036479500891265594)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n('GB', '\\t--> ', 0.054604872251931079)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n('ET', '\\t--> ', 1.0)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n('DT', '\\t--> ', 0.27811348781937012)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n('QD', '\\t--> ', 1.0)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n('LD', '\\t--> ', 1.0)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n('KN', '\\t--> ', 1.0)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n('SVC', '\\t--> ', 0.21240344622697563)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# meta-estimator\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    'KN': KNeighborsClassifier(3),\n",
    "    'SVC': SVC(kernel=\"linear\", C=0.025),\n",
    "    'SVC': SVC(gamma=2, C=1),\n",
    "    'DT': DecisionTreeClassifier(max_depth=5),\n",
    "    'RF': RandomForestClassifier(n_estimators=10, max_depth=5, max_features=1),  \n",
    "    'ET': ExtraTreesClassifier(n_estimators=10, max_depth=None),  \n",
    "    'AB': AdaBoostClassifier(n_estimators=100),\n",
    "    'GB': GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0), # clf.feature_importances_\n",
    "    'GNB': GaussianNB(),\n",
    "    'LD': LinearDiscriminantAnalysis(),\n",
    "    'QD': QuadraticDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "    \n",
    "    \n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100, random_state=0)\n",
    "\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    scores = cross_val_score(clf, X, y)\n",
    "    print(name,'\\t--> ',scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\nAccuracy: 0.93 (+/- 0.05) [Random Forest]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nAccuracy: 0.91 (+/- 0.04) [naive Bayes]\nAccuracy: 0.95 (+/- 0.05) [Ensemble]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9d0a968b1c50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# 设置参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m param_dist = {\"max_depth\": [3, None],\n\u001b[0;32m---> 47\u001b[0;31m               \u001b[0;34m\"max_features\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msp_randint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m               \u001b[0;34m\"min_samples_split\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msp_randint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m               \u001b[0;34m\"min_samples_leaf\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msp_randint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# integrated classifyer ,Bagging, Voting, GridSearch, PipeLine\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Bagging\n",
    "meta_clf = KNeighborsClassifier() \n",
    "bg_clf = BaggingClassifier(meta_clf, max_samples=0.5, max_features=0.5)\n",
    "\n",
    "#voting\n",
    "from sklearn import datasets\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard', weights=[2,1,2])\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_validation.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    \n",
    "#GridSearch\n",
    "\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# 生成数据\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# 元分类器\n",
    "meta_clf = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "# 设置参数\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(1, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# 运行随机搜索 RandomizedSearch\n",
    "n_iter_search = 20\n",
    "rs_clf = RandomizedSearchCV(meta_clf, param_distributions=param_dist,n_iter=n_iter_search)\n",
    "\n",
    "start = time()\n",
    "rs_clf.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "print(rs_clf.grid_scores_)\n",
    "\n",
    "\n",
    "# 设置参数\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [1, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# 运行网格搜索 GridSearch\n",
    "gs_clf = GridSearchCV(meta_clf, param_grid=param_grid)\n",
    "start = time()\n",
    "gs_clf.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(gs_clf.grid_scores_)))\n",
    "print(gs_clf.grid_scores_)\n",
    "\n",
    "#Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import samples_generator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 生成数据\n",
    "X, y = samples_generator.make_classification(n_informative=5, n_redundant=0, random_state=42)\n",
    "\n",
    "# 定义Pipeline，先方差分析，再SVM\n",
    "anova_filter = SelectKBest(f_regression, k=5)\n",
    "clf = svm.SVC(kernel='linear')\n",
    "pipe = Pipeline([('anova', anova_filter), ('svc', clf)])\n",
    "\n",
    "# 设置anova的参数k=10，svc的参数C=0.1（用双下划线\"__\"连接！）\n",
    "pipe.set_params(anova__k=10, svc__C=.1)\n",
    "pipe.fit(X, y)\n",
    "\n",
    "prediction = pipe.predict(X)\n",
    "\n",
    "pipe.score(X, y)                        \n",
    "\n",
    "# 得到 anova_filter 选出来的特征\n",
    "s = pipe.named_steps['anova'].get_support()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}